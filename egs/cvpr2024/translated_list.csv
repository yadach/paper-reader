title,authers,url,entry_id,pdf_url,summary,summary_ja
UniMODE: Unified Monocular 3D Object Detection,Zhuoling Li · Xiaogang Xu · Ser-Nam Lim · Hengshuang Zhao,,,,,
GAFusion: Adaptive Fusing LiDAR and Camera with Multiple Guidance for 3D Object Detection,Xiaotian Li · Baojie Fan · Jiandong Tian · Huijie Fan,,,,,
BEVSpread: Spread Voxel Pooling for Bird’s-Eye-View Representation in Vision-based Roadside 3D Object Detection,Wenjie Wang · Yehao Lu · Guangcong Zheng · Shuigenzhan · Xiaoqing Ye · Zichang Tan · Zichang Tan · Jingdong Wang · Gaoang Wang · Xi Li,,,,,
Towards Robust 3D Object Detection with LiDAR and 4D Radar Fusion in Various Weather Conditions,Yujeong Chae · Hyeonseong Kim · Kuk-Jin Yoon,,,,,
Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors,Haoxuanye Ji · Pengpeng Liang · Erkang Cheng,,http://arxiv.org/abs/2403.06093v1,http://arxiv.org/pdf/2403.06093v1,"Multi-camera-based 3D object detection has made notable progress in the past several years. However, we observe that there are cases (e.g. faraway regions) in which popular 2D object detectors are more reliable than state-of-the-art 3D detectors. In this paper, to improve the performance of query-based 3D object detectors, we present a novel query generating approach termed QAF2D, which infers 3D query anchors from 2D detection results. A 2D bounding box of an object in an image is lifted to a set of 3D anchors by associating each sampled point within the box with depth, yaw angle, and size candidates. Then, the validity of each 3D anchor is verified by comparing its projection in the image with its corresponding 2D box, and only valid anchors are kept and used to construct queries. The class information of the 2D bounding box associated with each query is also utilized to match the predicted boxes with ground truth for the set-based loss. The image feature extraction backbone is shared between the 3D detector and 2D detector by adding a small number of prompt parameters. We integrate QAF2D into three popular query-based 3D object detectors and carry out comprehensive evaluations on the nuScenes dataset. The largest improvement that QAF2D can bring about on the nuScenes validation subset is $2.3\%$ NDS and $2.7\%$ mAP. Code is available at https://github.com/nullmax-vision/QAF2D.",過去数年間で、マルチカメラベースの3D物体検出技術は notable な進歩を遂げてきました。しかし、我々は一部のケース（例：遠方の領域など）において、2D物体検出器が最新の3D検出器よりも信頼性が高いことを観察しています。本論文では、クエリベースの3D物体検出器の性能を向上させるために、2D検出結果から3Dクエリアンカーを推論する新しいクエリ生成アプローチであるQAF2Dを提案します。画像内のオブジェクトの2Dバウンディングボックスを、ボックス内の各サンプリングされたポイントを深さ、ヨー角、サイズの候補と関連付けることで、3Dアンカーのセットに昇格させます。その後、各3Dアンカーの妥当性を、画像内での射影と対応する2Dボックスと比較することによって検証し、有効なアンカーのみが保持されクエリの構築に使用されます。各クエリに関連付けられた2Dバウンディングボックスのクラス情報も、セットベースの損失のための予測ボックスとグラウンドトゥルースをマッチングするために利用されます。画像の特徴抽出バックボーンは、わずかなプロンプトパラメータを追加することで、3D検出器と2D検出器間で共有されます。QAF2Dを3つの人気のあるクエリベースの3D物体検出器に統合し、nuScenesデータセットで包括的な評価を実施します。QAF2DがnuScenes検証サブセットにもたらす最も大きな改善は、NDSで約2.3％、mAPで約2.7％です。コードは、https://github.com/nullmax-vision/QAF2D で入手可能です。
Multi-View Attentive Contextualization for Multi-View 3D Object Detection,Xianpeng Liu · Ce Zheng · Ming Qian · Nan Xue · Chen Chen · Zhebin Zhang · Chen Li · Tianfu Wu,,,,,
Weak-to-Strong 3D Object Detection with X-Ray Distillation,Alexander Gambashidze · Aleksandr Dadukin · Maksim Golyadkin · Maria Razzhivina · Ilya Makarov,,http://arxiv.org/abs/2404.00679v1,http://arxiv.org/pdf/2404.00679v1,"This paper addresses the critical challenges of sparsity and occlusion in LiDAR-based 3D object detection. Current methods often rely on supplementary modules or specific architectural designs, potentially limiting their applicability to new and evolving architectures. To our knowledge, we are the first to propose a versatile technique that seamlessly integrates into any existing framework for 3D Object Detection, marking the first instance of Weak-to-Strong generalization in 3D computer vision. We introduce a novel framework, X-Ray Distillation with Object-Complete Frames, suitable for both supervised and semi-supervised settings, that leverages the temporal aspect of point cloud sequences. This method extracts crucial information from both previous and subsequent LiDAR frames, creating Object-Complete frames that represent objects from multiple viewpoints, thus addressing occlusion and sparsity. Given the limitation of not being able to generate Object-Complete frames during online inference, we utilize Knowledge Distillation within a Teacher-Student framework. This technique encourages the strong Student model to emulate the behavior of the weaker Teacher, which processes simple and informative Object-Complete frames, effectively offering a comprehensive view of objects as if seen through X-ray vision. Our proposed methods surpass state-of-the-art in semi-supervised learning by 1-1.5 mAP and enhance the performance of five established supervised models by 1-2 mAP on standard autonomous driving datasets, even with default hyperparameters. Code for Object-Complete frames is available here: https://github.com/sakharok13/X-Ray-Teacher-Patching-Tools.",この論文は、LiDARを利用した3Dオブジェクト検出における疎な状況と遮蔽の課題に焦点を当てています。現在の手法は、しばしば補助モジュールや特定のアーキテクチャ設計に依存しており、新しいアーキテクチャへの適用範囲が限られる可能性があります。私たちの知る限り、強弱の一般化が3Dコンピュータビジョンにおいて初めて実現される「弱→強」の例であり、既存の任意のフレームワークにシームレスに統合される汎用的な手法を提案しています。我々は、3Dオブジェクト検出のための新しいフレームワークである『X-Ray Distillation with Object-Complete Frames』を導入しました。これは、監督・半監督の両環境に適した手法で、点群シーケンスの時間的側面を最大限に活用します。この手法は、過去および未来のLiDARフレームから重要な情報を抽出し、複数の視点からオブジェクトを表現するObject-Completeフレームを作成することにより、遮蔽と疎を克服します。オンライン推論中にObject-Completeフレームを生成できないという制約を考慮して、Teacher-Studentフレームワーク内でKnowledge Distillationを使用しています。この手法により、強力なStudentモデルが、単純で情報量の多いObject-Completeフレームを処理する弱いTeacherの振る舞いを模倣するよう促すことで、オブジェクトをX線ビジョンを介して捉えたかのような包括的な視点を提供します。提案された手法は、半監督学習において最先端の手法よりも1-1.5 mAP優れ、標準の自動運転データセットにおいて5つの確立された監督学習モデルの性能をデフォルトのハイパーパラメータのままでも1-2 mAP向上させています。Object-Completeフレームのコードはこちらで入手可能です： https://github.com/sakharok13/X-Ray-Teacher-Patching-Tools.
VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection,Zihua Liu · Hiroki Sakuma · Masatoshi Okutomi,http://www.ok.sc.e.titech.ac.jp/res/VSRD/index.html,http://arxiv.org/abs/2404.00149v1,http://arxiv.org/pdf/2404.00149v1,"Monocular 3D object detection poses a significant challenge in 3D scene understanding due to its inherently ill-posed nature in monocular depth estimation. Existing methods heavily rely on supervised learning using abundant 3D labels, typically obtained through expensive and labor-intensive annotation on LiDAR point clouds. To tackle this problem, we propose a novel weakly supervised 3D object detection framework named VSRD (Volumetric Silhouette Rendering for Detection) to train 3D object detectors without any 3D supervision but only weak 2D supervision. VSRD consists of multi-view 3D auto-labeling and subsequent training of monocular 3D object detectors using the pseudo labels generated in the auto-labeling stage. In the auto-labeling stage, we represent the surface of each instance as a signed distance field (SDF) and render its silhouette as an instance mask through our proposed instance-aware volumetric silhouette rendering. To directly optimize the 3D bounding boxes through rendering, we decompose the SDF of each instance into the SDF of a cuboid and the residual distance field (RDF) that represents the residual from the cuboid. This mechanism enables us to optimize the 3D bounding boxes in an end-to-end manner by comparing the rendered instance masks with the ground truth instance masks. The optimized 3D bounding boxes serve as effective training data for 3D object detection. We conduct extensive experiments on the KITTI-360 dataset, demonstrating that our method outperforms the existing weakly supervised 3D object detection methods. The code is available at https://github.com/skmhrk1209/VSRD.",単眼 3D オブジェクト検出は、単眼深度推定における本質的な不十分な性質に起因するため、3D シーン理解において重要な課題を提起します。既存の手法は、一般的には高価で労力を要する LiDAR ポイントクラウド上での豊富な 3D ラベルによる監督学習に大きく依存しています。この問題に取り組むため、我々は、3D 監督を必要とせず、弱い 2D 監督のみを用いた新しい弱監督型 3D オブジェクト検出フレームワークである VSRD（Volumetric Silhouette Rendering for Detection）を提案します。VSRD は、多方向から得られる 3D 自動ラベリングと、自動ラベリング段階で生成された疑似ラベルを用いた単眼 3D オブジェクト検出器の後続トレーニングで構成されています。自動ラベリング段階では、各インスタンスの表面を符号付き距離フィールド（SDF）として表現し、当該インスタンスに対するインスタンス感知型体積シルエットレンダリングによってそのシルエットをレンダリングします。レンダリングを通じて直接 3D バウンディングボックスを最適化するために、各インスタンスの SDF を直方体の SDF と直方体からの残差を表現する残差距離フィールド（RDF）に分解します。このメカニズムにより、レンダリングされたインスタンスマスクをグラウンドトゥルースのインスタンスマスクと比較することで、3D バウンディングボックスをエンドツーエンドで最適化することが可能となります。最適化された 3D バウンディングボックスは、3D オブジェクト検出のための有効なトレーニングデータとなります。我々は、KITTI-360 データセットで広範な実験を行い、この方法が既存の弱監督型 3D オブジェクト検出手法を上回ることを示しています。コードは https://github.com/skmhrk1209/VSRD で入手可能です。
PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection,Kuan-Chih Huang · Weijie Lyu · Ming-Hsuan Yang · Yi-Hsuan Tsai,,,,,
Decoupled Pseudo-labeling in Semi-Supervised Monocular 3D Object Detection,Jiacheng Zhang · Jiaming Li · Xiangru Lin · Wei Zhang · Xiao Tan · Junyu Han · Errui Ding · Jingdong Wang · Guanbin Li,,,,,
Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-dataset 3D Object Detection,Zhanwei Zhang · Minghao Chen · Shuai Xiao · Liang Peng · Hengjia Li · Binbin Lin · Ping Li · Wenxiao Wang · Boxi Wu · Deng Cai,,http://arxiv.org/abs/2404.19384v1,http://arxiv.org/pdf/2404.19384v1,"Recent self-training techniques have shown notable improvements in unsupervised domain adaptation for 3D object detection (3D UDA). These techniques typically select pseudo labels, i.e., 3D boxes, to supervise models for the target domain. However, this selection process inevitably introduces unreliable 3D boxes, in which 3D points cannot be definitively assigned as foreground or background. Previous techniques mitigate this by reweighting these boxes as pseudo labels, but these boxes can still poison the training process. To resolve this problem, in this paper, we propose a novel pseudo label refinery framework. Specifically, in the selection process, to improve the reliability of pseudo boxes, we propose a complementary augmentation strategy. This strategy involves either removing all points within an unreliable box or replacing it with a high-confidence box. Moreover, the point numbers of instances in high-beam datasets are considerably higher than those in low-beam datasets, also degrading the quality of pseudo labels during the training process. We alleviate this issue by generating additional proposals and aligning RoI features across different domains. Experimental results demonstrate that our method effectively enhances the quality of pseudo labels and consistently surpasses the state-of-the-art methods on six autonomous driving benchmarks. Code will be available at https://github.com/Zhanwei-Z/PERE.",最近の独学技術は、3Dオブジェクト検出（3D UDA）のための教師なしドメイン適応において著しい改善を示しています。これらの技術は通常、擬似ラベル（すなわち、3Dボックス）を選択して、ターゲットドメインのモデルを監督します。ただし、この選択プロセスによって必然的に信頼性のない3Dボックスが導入され、3Dポイントを明確に前景または背景として割り当てられない場合があります。以前の技術では、これらのボックスを擬似ラベルとして再重み付けすることで、これを緩和していましたが、これらのボックスはトレーニングプロセスに有害な要因となる可能性があります。この問題を解決するために、本論文では、新しい擬似ラベルリファイナリーフレームワークを提案します。具体的には、信頼性の高い擬似ボックスを改善するために、選択プロセスにおいて、補完的な強化戦略を提案します。この戦略では、信頼性の低いボックス内のすべてのポイントを削除するか、高信頼ボックスで置き換えます。さらに、高ビームデータセット内のインスタンスのポイント数は、低ビームデータセット内のそれよりもかなり大きく、トレーニングプロセス中の擬似ラベルの品質も低下します。この問題を解決するために、追加提案を生成し、異なるドメイン間でRoI特徴を整列させます。実験結果は、当社の手法が擬似ラベルの品質を効果的に向上させ、自動運転ベンチマークの6つで常に最新技術を上回ることを示しています。コードは https://github.com/Zhanwei-Z/PERE で入手できます。
RCBEVDet: Radar-camera Fusion in Bird’s Eye View for 3D Object Detection,Zhiwei Lin · Zhe Liu · Zhongyu Xia · Xinhao Wang · Yongtao Wang · Shengxiang Qi · Yang Dong · Nan Dong · Le Zhang · Ce Zhu,,,,,
$MonoDiff$: Monocular 3D Object Detection and Pose Estimation with Diffusion Models,Yasiru Ranasinghe · Deepti Hegde · Vishal M. Patel,,,,,
IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection,Junbo Yin · Wenguan Wang · Runnan Chen · Wei Li · Ruigang Yang · Pascal Frossard · Jianbing Shen,,http://arxiv.org/abs/2403.15241v1,http://arxiv.org/pdf/2403.15241v1,"Bird's eye view (BEV) representation has emerged as a dominant solution for describing 3D space in autonomous driving scenarios. However, objects in the BEV representation typically exhibit small sizes, and the associated point cloud context is inherently sparse, which leads to great challenges for reliable 3D perception. In this paper, we propose IS-Fusion, an innovative multimodal fusion framework that jointly captures the Instance- and Scene-level contextual information. IS-Fusion essentially differs from existing approaches that only focus on the BEV scene-level fusion by explicitly incorporating instance-level multimodal information, thus facilitating the instance-centric tasks like 3D object detection. It comprises a Hierarchical Scene Fusion (HSF) module and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Grid and Grid-to-Region transformers to capture the multimodal scene context at different granularities. IGF mines instance candidates, explores their relationships, and aggregates the local multimodal context for each instance. These instances then serve as guidance to enhance the scene feature and yield an instance-aware BEV representation. On the challenging nuScenes benchmark, IS-Fusion outperforms all the published multimodal works to date. Code is available at: https://github.com/yinjunbo/IS-Fusion.",バードズアイビュー（BEV）表現は、自律走行シナリオにおける3D空間を記述するための主要な解決策として登場しました。しかし、BEV表現におけるオブジェクトは通常、小さなサイズを示し、関連するポイントクラウドコンテキストが本質的にまばらであるため、信頼性の高い3D認識には大きな課題が生じます。本論文では、Instance-およびSceneレベルのコンテクスト情報を共に捉える革新的なマルチモーダルフュージョンフレームワークであるIS-Fusionを提案します。IS-Fusionは、従来のBEVシーンレベルフュージョンに焦点を当てる既存のアプローチとは異なり、明示的にインスタンスレベルのマルチモーダル情報を組み込むことによって、3Dオブジェクト検出などのインスタンス中心のタスクを容易にします。それは、Hierarchical Scene Fusion（HSF）モジュールとInstance-Guided Fusion（IGF）モジュールで構成されています。HSFは、Point-to-GridとGrid-to-Regionのトランスフォーマーを適用して、異なる粒度でのマルチモーダルシーンコンテキストを捉えます。IGFは、インスタンス候補を探索し、その関係を探求し、各インスタンスのローカルなマルチモーダルコンテキストを集約します。これらのインスタンスは、シーン特徴を強化し、インスタンスに対応するBEV表現を提供するためのガイダンスとなります。挑戦的なnuScenesベンチマークでは、IS-Fusionはこれまでに公開されたすべてのマルチモーダル作品を上回っています。コードはこちらから入手できます: https://github.com/yinjunbo/IS-Fusion.
CaKDP: Category-aware Knowledge Distillation and Pruning Framework for Lightweight 3D Object Detection,Haonan Zhang · Longjun Liu · Yuqi Huang · YangZhao · Xinyu Lei · Bihan Wen,,,,,
SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection,Gang Zhang · Chen Junnan · Guohuan Gao · Jianmin Li · Si Liu · Xiaolin Hu,,,,,
Learning Occupancy for Monocular 3D Object Detection,Liang Peng · Junkai Xu · Haoran Cheng · Zheng Yang · Xiaopei Wu · Wei Qian · Wenxiao Wang · Boxi Wu · Deng Cai,,http://arxiv.org/abs/2305.15694v1,http://arxiv.org/pdf/2305.15694v1,"Monocular 3D detection is a challenging task due to the lack of accurate 3D information. Existing approaches typically rely on geometry constraints and dense depth estimates to facilitate the learning, but often fail to fully exploit the benefits of three-dimensional feature extraction in frustum and 3D space. In this paper, we propose \textbf{OccupancyM3D}, a method of learning occupancy for monocular 3D detection. It directly learns occupancy in frustum and 3D space, leading to more discriminative and informative 3D features and representations. Specifically, by using synchronized raw sparse LiDAR point clouds, we define the space status and generate voxel-based occupancy labels. We formulate occupancy prediction as a simple classification problem and design associated occupancy losses. Resulting occupancy estimates are employed to enhance original frustum/3D features. As a result, experiments on KITTI and Waymo open datasets demonstrate that the proposed method achieves a new state of the art and surpasses other methods by a significant margin. Codes and pre-trained models will be available at \url{https://github.com/SPengLiang/OccupancyM3D}.",モノキュラー3D検出は、正確な3D情報の欠如のために困難な課題です。既存のアプローチは、通常、ジオメトリ制約と密な深度推定に依存して学習を促進しますが、しばしばフラスタムと3D空間における立体特徴抽出の利点を十分に活かせません。本論文では、モノキュラー3D検出のための占有学習手法である\textbf{OccupancyM3D}を提案します。これにより、フラスタムと3D空間において直接占有を学習し、より識別的で情報量の豊かな3D特徴と表現を導出します。具体的には、同期した生のスパースLiDARポイントクラウドを使用して、空間の状態を定義し、ボクセルベースの占有ラベルを生成します。占有予測を単純な分類問題として定式化し、関連する占有損失を設計します。結果として導出された占有推定値は、元のフラスタム/3D特徴を強化するために使用されます。その結果、KITTIとWaymoのオープンデータセットでの実験により、提案手法が他の手法を大きく超えて最先端の成果を達成することが示されました。コードと事前学習済みモデルは、\url{https://github.com/SPengLiang/OccupancyM3D}で入手できます。
Prompt3D: Random Prompt Assisted Weakly-Supervised 3D Object Detection,Xiaohong Zhang · Huisheng Ye · Jingwen Li · Qinyu Tang · Yuanqi Li · Yanwen Guo · Jie Guo,,,,,
MonoCD: Monocular 3D Object Detection with Complementary Depths,Longfei Yan · Pei Yan · Shengzhou Xiong · Xuanyu Xiang · Yihua Tan,,http://arxiv.org/abs/2404.03181v1,http://arxiv.org/pdf/2404.03181v1,"Monocular 3D object detection has attracted widespread attention due to its potential to accurately obtain object 3D localization from a single image at a low cost. Depth estimation is an essential but challenging subtask of monocular 3D object detection due to the ill-posedness of 2D to 3D mapping. Many methods explore multiple local depth clues such as object heights and keypoints and then formulate the object depth estimation as an ensemble of multiple depth predictions to mitigate the insufficiency of single-depth information. However, the errors of existing multiple depths tend to have the same sign, which hinders them from neutralizing each other and limits the overall accuracy of combined depth. To alleviate this problem, we propose to increase the complementarity of depths with two novel designs. First, we add a new depth prediction branch named complementary depth that utilizes global and efficient depth clues from the entire image rather than the local clues to reduce the correlation of depth predictions. Second, we propose to fully exploit the geometric relations between multiple depth clues to achieve complementarity in form. Benefiting from these designs, our method achieves higher complementarity. Experiments on the KITTI benchmark demonstrate that our method achieves state-of-the-art performance without introducing extra data. In addition, complementary depth can also be a lightweight and plug-and-play module to boost multiple existing monocular 3d object detectors. Code is available at https://github.com/elvintanhust/MonoCD.",単眼3Dオブジェクト検出は、1枚の画像からオブジェクトの3D位置情報を低コストで正確に取得する可能性があるため、広範な注目を集めています。深度推定は単眼3Dオブジェクト検出の重要でありながら困難なサブタスクであり、2Dから3Dへのマッピングの不適切さに起因しています。多くの手法は、オブジェクトの高さやキーポイントなどを含む複数の局所的な深度手掛かりを探索し、単一深度情報の不十分さを緩和するために複数の深度予測をまとめたオブジェクトの深度推定として形成しています。しかし、既存の複数の深度のエラーは同じ符号を持つ傾向があり、互いを相殺することを妨げ、組み合わせた深度の総合的な精度を制限しています。この問題を緩和するために、私たちは2つの新しい設計を使用して深度の補完性を高めることを提案します。まず、画像全体からローカルな手掛かりではなく、グローバルかつ効率的な深度手掛かりを利用する補完的な深度という新しい深度予測ブランチを追加し、深度予測の相関を減少させることを提案します。次に、複数の深度手掛かり間の幾何学的関係を十分に利用して形式的補完性を達成することを提案します。これらの設計の恩恵を受けることで、当社の手法はより高い補完性を達成します。KITTIベンチマークでの実験結果は、当社の手法が追加データを導入することなく最先端の性能を達成していることを示しています。さらに、補完的な深度は、既存の単眼3Dオブジェクト検出器を強化するための軽量なプラグアンドプレイモジュールとしても利用できます。コードはhttps://github.com/elvintanhust/MonoCD で入手可能です。
BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection,Zhenxin Li · Shiyi Lan · Jose M. Alvarez · Zuxuan Wu,,http://arxiv.org/abs/2312.01696v2,http://arxiv.org/pdf/2312.01696v2,"Recently, the rise of query-based Transformer decoders is reshaping camera-based 3D object detection. These query-based decoders are surpassing the traditional dense BEV (Bird's Eye View)-based methods. However, we argue that dense BEV frameworks remain important due to their outstanding abilities in depth estimation and object localization, depicting 3D scenes accurately and comprehensively. This paper aims to address the drawbacks of the existing dense BEV-based 3D object detectors by introducing our proposed enhanced components, including a CRF-modulated depth estimation module enforcing object-level consistencies, a long-term temporal aggregation module with extended receptive fields, and a two-stage object decoder combining perspective techniques with CRF-modulated depth embedding. These enhancements lead to a ""modernized"" dense BEV framework dubbed BEVNeXt. On the nuScenes benchmark, BEVNeXt outperforms both BEV-based and query-based frameworks under various settings, achieving a state-of-the-art result of 64.2 NDS on the nuScenes test set. Code will be available at \url{https://github.com/woxihuanjiangguo/BEVNeXt}.",最近、問い合わせベースのTransformerデコーダーの台頭が、カメラベースの3Dオブジェクト検出を再構築しています。これらの問い合わせベースのデコーダーは、従来の密なBEV（鳥瞰図）ベースの手法を凌駕しています。しかし、私たちは、密なBEVフレームワークが深度推定やオブジェクトの位置特定において優れた能力を持つため、3Dシーンを正確かつ包括的に描写する点で重要であると主張します。この論文は、既存の密なBEVベースの3Dオブジェクト検出器の欠点に対処することを目的とし、CRFモジュールでオブジェクトレベルの一貫性を強化する深度推定モジュール、拡張された受容野を持つ長期的な時間集約モジュール、そして透視技術とCRFモジュールで深度埋め込みを組み合わせた2段階オブジェクトデコーダーなど、提案された強化コンポーネントを導入します。これらの強化機能により、「近代化された」密なBEVフレームワークであるBEVNeXtが生まれます。nuScenesベンチマークでは、BEVNeXtはさまざまな設定でBEVベースおよび問い合わせベースのフレームワークを凌駕し、nuScenesテストセットで64.2のNDSという最先端の結果を達成しています。コードはこちらで入手可能：\url{https://github.com/woxihuanjiangguo/BEVNeXt}。
3DiffTection: 3D Object Detection with Geometry-aware Diffusion Features,Chenfeng Xu · Huan Ling · Sanja Fidler · Or Litany,,,,,
Commonsense Prototype for Outdoor Unsupervised 3D Object Detection,Hai Wu · Shijia Zhao · Xun Huang · Chenglu Wen · Xin Li · Cheng Wang,,http://arxiv.org/abs/2404.16493v1,http://arxiv.org/pdf/2404.16493v1,"The prevalent approaches of unsupervised 3D object detection follow cluster-based pseudo-label generation and iterative self-training processes. However, the challenge arises due to the sparsity of LiDAR scans, which leads to pseudo-labels with erroneous size and position, resulting in subpar detection performance. To tackle this problem, this paper introduces a Commonsense Prototype-based Detector, termed CPD, for unsupervised 3D object detection. CPD first constructs Commonsense Prototype (CProto) characterized by high-quality bounding box and dense points, based on commonsense intuition. Subsequently, CPD refines the low-quality pseudo-labels by leveraging the size prior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely scanned objects by the geometric knowledge from CProto. CPD outperforms state-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD), PandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD and testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on easy and moderate car classes, respectively. These achievements position CPD in close proximity to fully supervised detectors, highlighting the significance of our method. The code will be available at https://github.com/hailanyi/CPD.",非監督3Dオブジェクト検知の主流のアプローチは、クラスターをベースとする疑似ラベル生成と反復的な自己学習プロセスに従います。しかし、LiDARスキャンの希薄さにより誤ったサイズや位置を持つ疑似ラベルが生成され、検知性能が低下するという課題が生じます。この問題に取り組むために、本論文では、Commonsense Prototype-based Detector（CPD）と呼ばれる非監督3Dオブジェクト検知手法を紹介します。CPDは、まず、コモンセンス的直感に基づいて、高品質のバウンディングボックスと密なポイントに特徴づけられるCommonsense Prototype（CProto）を構築します。その後、CPDはCProtoからサイズ事前情報を活用して低品質の疑似ラベルを改良します。さらに、CPDはCProtoからの幾何学的知識により、希薄にスキャンされたオブジェクトの検知精度を向上させます。CPDは、Waymo Open Dataset（WOD）、PandaSet、およびKITTIデータセットの最新の非監督3D検出器を大幅に上回る性能を発揮します。また、WODでの訓練とKITTIでのテストを行うことで、CPDは容易および中程度の車両カテゴリーでそれぞれ90.85％と81.01％の3D平均精度を達成します。これらの成果は、CPDを完全に監視された検出器に近い位置に配置し、当該手法の重要性を強調しています。コードはhttps://github.com/hailanyi/CPDで入手できます。
HINTED: Hard Instance Enhanced Detector with Mixed-Density Feature Fusion for Sparsely-Supervised 3D Object Detection,Qiming Xia · Wei Ye · Hai Wu · Shijia Zhao · Leyuan Xing · Xun Huang · Jinhao Deng · Xin Li · Chenglu Wen · Cheng Wang,,,,,
Improving Distant 3D Object Detection Using 2D Box Supervision,Zetong Yang · Zhiding Yu · Christopher Choy · Renhao Wang · Anima Anandkumar · Jose M. Alvarez,,http://arxiv.org/abs/2403.09230v1,http://arxiv.org/pdf/2403.09230v1,"Improving the detection of distant 3d objects is an important yet challenging task. For camera-based 3D perception, the annotation of 3d bounding relies heavily on LiDAR for accurate depth information. As such, the distance of annotation is often limited due to the sparsity of LiDAR points on distant objects, which hampers the capability of existing detectors for long-range scenarios. We address this challenge by considering only 2D box supervision for distant objects since they are easy to annotate. We propose LR3D, a framework that learns to recover the missing depth of distant objects. LR3D adopts an implicit projection head to learn the generation of mapping between 2D boxes and depth using the 3D supervision on close objects. This mapping allows the depth estimation of distant objects conditioned on their 2D boxes, making long-range 3D detection with 2D supervision feasible. Experiments show that without distant 3D annotations, LR3D allows camera-based methods to detect distant objects (over 200m) with comparable accuracy to full 3D supervision. Our framework is general, and could widely benefit 3D detection methods to a large extent.",遠隔の3Dオブジェクトの検出を向上させることは重要でありながらも挑戦的な課題です。カメラベースの3D知覚において、3Dバウンディングボックスのアノテーションは正確な深度情報のためにLiDARに大きく依存しています。そのため、遠隔オブジェクトに対するアノテーションの距離は、遠くのオブジェクト上のLiDARポイントの疎さにより制限されることが多く、これは長距離シナリオにおける既存の検出器の能力を妨げます。私たちは、遠隔のオブジェクトについては2Dボックスに対する監督のみを考慮することで、この課題に取り組んでいます。遠隔オブジェクトの深度の復元を学習するLR3Dというフレームワークを提案しています。LR3Dは、2Dボックスと距離との間のマッピング生成を学習するために暗黙的な射影ヘッドを採用し、近くのオブジェクト上の3D監督を使用します。このマッピングにより、2Dボックスに依存した遠隔オブジェクトの深度推定が可能となり、2D監督による長距離3D検出が実現可能となります。実験では、遠隔の3Dアノテーションがない状況でも、LR3Dにより、カメラベースの手法が遠隔のオブジェクト（200m以上）を全体の3D監督と同等の精度で検出できることが示されています。私たちのフレームワークは汎用性があり、広範囲にわたる3D検出方法に大きな利益をもたらす可能性があります。
