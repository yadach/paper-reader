title,authers,url,entry_id,pdf_url,summary,summary_ja
UniMODE: Unified Monocular 3D Object Detection,Zhuoling Li · Xiaogang Xu · Ser-Nam Lim · Hengshuang Zhao,,,,,
GAFusion: Adaptive Fusing LiDAR and Camera with Multiple Guidance for 3D Object Detection,Xiaotian Li · Baojie Fan · Jiandong Tian · Huijie Fan,,,,,
BEVSpread: Spread Voxel Pooling for Bird’s-Eye-View Representation in Vision-based Roadside 3D Object Detection,Wenjie Wang · Yehao Lu · Guangcong Zheng · Shuigenzhan · Xiaoqing Ye · Zichang Tan · Zichang Tan · Jingdong Wang · Gaoang Wang · Xi Li,,,,,
Towards Robust 3D Object Detection with LiDAR and 4D Radar Fusion in Various Weather Conditions,Yujeong Chae · Hyeonseong Kim · Kuk-Jin Yoon,,,,,
Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors,Haoxuanye Ji · Pengpeng Liang · Erkang Cheng,,http://arxiv.org/abs/2403.06093v1,http://arxiv.org/pdf/2403.06093v1,"Multi-camera-based 3D object detection has made notable progress in the past several years. However, we observe that there are cases (e.g. faraway regions) in which popular 2D object detectors are more reliable than state-of-the-art 3D detectors. In this paper, to improve the performance of query-based 3D object detectors, we present a novel query generating approach termed QAF2D, which infers 3D query anchors from 2D detection results. A 2D bounding box of an object in an image is lifted to a set of 3D anchors by associating each sampled point within the box with depth, yaw angle, and size candidates. Then, the validity of each 3D anchor is verified by comparing its projection in the image with its corresponding 2D box, and only valid anchors are kept and used to construct queries. The class information of the 2D bounding box associated with each query is also utilized to match the predicted boxes with ground truth for the set-based loss. The image feature extraction backbone is shared between the 3D detector and 2D detector by adding a small number of prompt parameters. We integrate QAF2D into three popular query-based 3D object detectors and carry out comprehensive evaluations on the nuScenes dataset. The largest improvement that QAF2D can bring about on the nuScenes validation subset is $2.3\%$ NDS and $2.7\%$ mAP. Code is available at https://github.com/nullmax-vision/QAF2D.",数年間、マルチカメラを基盤とした３次元物体検出技術は顕著な進歩を遂げてきました。しかし、一部の状況（例：遠方領域）では、主要な２次元物体検出器が最先端の３次元検出器よりも信頼性が高いということを観察しています。本論文では、クエリベースの３次元物体検出器の性能を向上させるために、QAF2Dという新しいクエリ生成アプローチを提案します。これは２次元の検出結果から３次元のクエリアンカーを推論するものです。画像内の物体の２次元のバウンディングボックスを、ボックス内の各サンプリング点と深度、ヨー角度、サイズ候補を関連付けることで、３次元のアンカーのセットに変換します。その後、各３次元アンカーが画像内での射影と対応する２次元ボックスを比較して、有効なアンカーのみを保持し、クエリを構築に使用します。各クエリに関連付けられた２次元バウンディングボックスのクラス情報を利用して、予測されたボックスをグラウンドトゥルースと一致させるためのセットベースの損失関数を適用します。３次元検出器と２次元検出器の画像特徴抽出バックボーンは、プロンプトパラメータを追加することで共有されています。私たちはQAF2Dを３つの人気のあるクエリベースの３次元物体検出器に統合し、nuScenesデータセットで包括的な評価を行いました。nuScenes検証サブセットでQAF2Dがもたらす最大の改善は、NDSで$2.3\%$、mAPで$2.7\%$です。コードは以下のリンクから入手可能です： https://github.com/nullmax-vision/QAF2D。
Multi-View Attentive Contextualization for Multi-View 3D Object Detection,Xianpeng Liu · Ce Zheng · Ming Qian · Nan Xue · Chen Chen · Zhebin Zhang · Chen Li · Tianfu Wu,,,,,
Weak-to-Strong 3D Object Detection with X-Ray Distillation,Alexander Gambashidze · Aleksandr Dadukin · Maksim Golyadkin · Maria Razzhivina · Ilya Makarov,,http://arxiv.org/abs/2404.00679v1,http://arxiv.org/pdf/2404.00679v1,"This paper addresses the critical challenges of sparsity and occlusion in LiDAR-based 3D object detection. Current methods often rely on supplementary modules or specific architectural designs, potentially limiting their applicability to new and evolving architectures. To our knowledge, we are the first to propose a versatile technique that seamlessly integrates into any existing framework for 3D Object Detection, marking the first instance of Weak-to-Strong generalization in 3D computer vision. We introduce a novel framework, X-Ray Distillation with Object-Complete Frames, suitable for both supervised and semi-supervised settings, that leverages the temporal aspect of point cloud sequences. This method extracts crucial information from both previous and subsequent LiDAR frames, creating Object-Complete frames that represent objects from multiple viewpoints, thus addressing occlusion and sparsity. Given the limitation of not being able to generate Object-Complete frames during online inference, we utilize Knowledge Distillation within a Teacher-Student framework. This technique encourages the strong Student model to emulate the behavior of the weaker Teacher, which processes simple and informative Object-Complete frames, effectively offering a comprehensive view of objects as if seen through X-ray vision. Our proposed methods surpass state-of-the-art in semi-supervised learning by 1-1.5 mAP and enhance the performance of five established supervised models by 1-2 mAP on standard autonomous driving datasets, even with default hyperparameters. Code for Object-Complete frames is available here: https://github.com/sakharok13/X-Ray-Teacher-Patching-Tools.",この論文は、LiDARを利用した3D物体検出におけるスパース性と遮蔽の重要な課題に取り組んでいます。現在の手法は、補助モジュールや特定のアーキテクチャ設計に依存することが多く、新しい進化するアーキテクチャへの適用範囲を制限する可能性があります。私たちの知る限り、3Dオブジェクト検出のための任意の既存フレームワークにシームレスに統合される多目的な手法を最初に提案したので、3DコンピュータビジョンにおけるWeak-to-Strongの一般化の最初の例となります。これには、点群シーケンスの時間的側面を活用する監督および半監督設定の両方に適した、新しいフレームワークであるObject-Complete Framesを備えたX-Ray Distillationを導入します。この手法は、過去および後続のLiDARフレームから重要な情報を抽出し、複数の視点からオブジェクトを表現するObject-Completeフレームを作成することで、遮蔽とスパース性に対処します。オンライン推論中にObject-Completeフレームを生成することができないという制限に鑑み、私たちはTeacher-Studentフレームワーク内でKnowledge Distillationを利用しています。この手法は、シンプルで情報豊かなObject-Completeフレームを処理する弱いTeacherの挙動を真の強いStudentモデルが模倣することを促し、X線ビジョンを通して見たかのようにオブジェクトの包括的な視点を提供します。私たちの提案手法は、半監督学習においてstate-of-the-artを1-1.5 mAP超え、標準の自動運転データセットにおいて、デフォルトのハイパーパラメータを使用しても5つの確立された監督モデルの性能を1-2 mAP向上させます。Object-Complete framesのコードはこちらで入手可能です：https://github.com/sakharok13/X-Ray-Teacher-Patching-Tools。
VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection,Zihua Liu · Hiroki Sakuma · Masatoshi Okutomi,http://www.ok.sc.e.titech.ac.jp/res/VSRD/index.html,http://arxiv.org/abs/2404.00149v1,http://arxiv.org/pdf/2404.00149v1,"Monocular 3D object detection poses a significant challenge in 3D scene understanding due to its inherently ill-posed nature in monocular depth estimation. Existing methods heavily rely on supervised learning using abundant 3D labels, typically obtained through expensive and labor-intensive annotation on LiDAR point clouds. To tackle this problem, we propose a novel weakly supervised 3D object detection framework named VSRD (Volumetric Silhouette Rendering for Detection) to train 3D object detectors without any 3D supervision but only weak 2D supervision. VSRD consists of multi-view 3D auto-labeling and subsequent training of monocular 3D object detectors using the pseudo labels generated in the auto-labeling stage. In the auto-labeling stage, we represent the surface of each instance as a signed distance field (SDF) and render its silhouette as an instance mask through our proposed instance-aware volumetric silhouette rendering. To directly optimize the 3D bounding boxes through rendering, we decompose the SDF of each instance into the SDF of a cuboid and the residual distance field (RDF) that represents the residual from the cuboid. This mechanism enables us to optimize the 3D bounding boxes in an end-to-end manner by comparing the rendered instance masks with the ground truth instance masks. The optimized 3D bounding boxes serve as effective training data for 3D object detection. We conduct extensive experiments on the KITTI-360 dataset, demonstrating that our method outperforms the existing weakly supervised 3D object detection methods. The code is available at https://github.com/skmhrk1209/VSRD.",モノキュラー3Dオブジェクト検出は、モノキュラー深度推定において本来不適切な性質から3Dシーン理解における著しい課題を提起します。既存の方法は一般的に、LiDARポイントクラウド上で高価かつ労力を要するアノテーションを通じて取得された豊富な3Dラベルを使用した教師あり学習に大きく依存しています。この問題に対処するため、我々はVSRD（Volumetric Silhouette Rendering for Detection）という新しい弱教師あり3Dオブジェクト検出フレームワークを提案しており、3D監督なしでの3Dオブジェクト検出器のトレーニングを2D弱教師のみを用いて行うことが可能です。VSRDは、多視点3D自動ラベリングと自動ラベリング段階で生成された擬似ラベルを使用してモノキュラー3Dオブジェクト検出器をトレーニングすることを可能にします。自動ラベリング段階では、各インスタンスの表面を符号付き距離場（SDF）として表現し、当該インスタンスを示すマスクを提案されたインスタンス認識のためのボリューメトリックシルエットレンダリングによって生成します。レンダリングを介して3D境界ボックスを直接最適化するために、各インスタンスのSDFを立体および残差距離フィールド（RDF）に分解します。このメカニズムにより、レンダリングされたインスタンスマスクをグラウンドトゥルーインスタンスマスクと比較することで、エンドツーエンドの方法で3D境界ボックスを最適化することが可能となります。最適化された3D境界ボックスは、3Dオブジェクト検出の効果的なトレーニングデータとして機能します。我々はKITTI-360データセットで幅広い実験を行い、弱教師あり3Dオブジェクト検出方法よりも我々の手法が優れていることを示しています。コードは https://github.com/skmhrk1209/VSRD で入手可能です。
PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection,Kuan-Chih Huang · Weijie Lyu · Ming-Hsuan Yang · Yi-Hsuan Tsai,,,,,
Decoupled Pseudo-labeling in Semi-Supervised Monocular 3D Object Detection,Jiacheng Zhang · Jiaming Li · Xiangru Lin · Wei Zhang · Xiao Tan · Junyu Han · Errui Ding · Jingdong Wang · Guanbin Li,,,,,
Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-dataset 3D Object Detection,Zhanwei Zhang · Minghao Chen · Shuai Xiao · Liang Peng · Hengjia Li · Binbin Lin · Ping Li · Wenxiao Wang · Boxi Wu · Deng Cai,,http://arxiv.org/abs/2404.19384v1,http://arxiv.org/pdf/2404.19384v1,"Recent self-training techniques have shown notable improvements in unsupervised domain adaptation for 3D object detection (3D UDA). These techniques typically select pseudo labels, i.e., 3D boxes, to supervise models for the target domain. However, this selection process inevitably introduces unreliable 3D boxes, in which 3D points cannot be definitively assigned as foreground or background. Previous techniques mitigate this by reweighting these boxes as pseudo labels, but these boxes can still poison the training process. To resolve this problem, in this paper, we propose a novel pseudo label refinery framework. Specifically, in the selection process, to improve the reliability of pseudo boxes, we propose a complementary augmentation strategy. This strategy involves either removing all points within an unreliable box or replacing it with a high-confidence box. Moreover, the point numbers of instances in high-beam datasets are considerably higher than those in low-beam datasets, also degrading the quality of pseudo labels during the training process. We alleviate this issue by generating additional proposals and aligning RoI features across different domains. Experimental results demonstrate that our method effectively enhances the quality of pseudo labels and consistently surpasses the state-of-the-art methods on six autonomous driving benchmarks. Code will be available at https://github.com/Zhanwei-Z/PERE.",最近のセルフトレーニング技術は、3D物体検出（3D UDA）における教師なしドメイン適応の顕著な改善を示しています。これらの技術は通常、擬似ラベル、つまり3Dボックスを選択して、ターゲットドメインのモデルを監督しています。ただし、この選択プロセスにより、3Dポイントを前景または背景に明確に割り当てることができない信頼性の低い3Dボックスが避けられません。以前の技術は、これらのボックスを擬似ラベルとして再重み付けすることで、この問題を緩和してきましたが、これらのボックスはトレーニングプロセスに依然として悪影響を及ぼす可能性があります。この問題を解決するために、本論文では新しい擬似ラベルのリファイナリーフレームワークを提案しています。具体的には、擬似ボックスの信頼性を向上させるために、補完的な拡張戦略を提案しています。この戦略は、信頼性の低いボックス内のすべてのポイントを削除するか、高信頼なボックスでそれを置き換えるということを含みます。さらに、ハイビームデータセット内のインスタンスのポイント数は低ビームデータセットよりもかなり高く、トレーニングプロセス中の擬似ラベルの品質を低下させることもあります。この問題を解決するために、追加の提案を生成し、異なるドメイン間でRoIフィーチャを整列させます。実験結果は、当社の手法が擬似ラベルの品質を効果的に向上させ、6つの自動運転ベンチマークで最新技術を常に上回ることを示しています。コードは次の場所で入手可能です：https://github.com/Zhanwei-Z/PERE.
RCBEVDet: Radar-camera Fusion in Bird’s Eye View for 3D Object Detection,Zhiwei Lin · Zhe Liu · Zhongyu Xia · Xinhao Wang · Yongtao Wang · Shengxiang Qi · Yang Dong · Nan Dong · Le Zhang · Ce Zhu,,,,,
$MonoDiff$: Monocular 3D Object Detection and Pose Estimation with Diffusion Models,Yasiru Ranasinghe · Deepti Hegde · Vishal M. Patel,,,,,
IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection,Junbo Yin · Wenguan Wang · Runnan Chen · Wei Li · Ruigang Yang · Pascal Frossard · Jianbing Shen,,http://arxiv.org/abs/2403.15241v1,http://arxiv.org/pdf/2403.15241v1,"Bird's eye view (BEV) representation has emerged as a dominant solution for describing 3D space in autonomous driving scenarios. However, objects in the BEV representation typically exhibit small sizes, and the associated point cloud context is inherently sparse, which leads to great challenges for reliable 3D perception. In this paper, we propose IS-Fusion, an innovative multimodal fusion framework that jointly captures the Instance- and Scene-level contextual information. IS-Fusion essentially differs from existing approaches that only focus on the BEV scene-level fusion by explicitly incorporating instance-level multimodal information, thus facilitating the instance-centric tasks like 3D object detection. It comprises a Hierarchical Scene Fusion (HSF) module and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Grid and Grid-to-Region transformers to capture the multimodal scene context at different granularities. IGF mines instance candidates, explores their relationships, and aggregates the local multimodal context for each instance. These instances then serve as guidance to enhance the scene feature and yield an instance-aware BEV representation. On the challenging nuScenes benchmark, IS-Fusion outperforms all the published multimodal works to date. Code is available at: https://github.com/yinjunbo/IS-Fusion.",鳥瞰図（BEV）表現は、自動運転シナリオで3D空間を記述するための主要なソリューションとして登場しています。しかし、BEV表現内のオブジェクトは通常小さなサイズであり、関連するポイントクラウドコンテキストは本質的に疎なため、信頼性の高い3D知覚には大きな課題が生じます。本論文では、Instance-およびScene-levelのコンテキスト情報を共に捉える革新的なマルチモーダル融合フレームワークであるIS-Fusionを提案します。IS-Fusionは、従来のBEVシーンレベルの融合に焦点を当てた手法とは基本的に異なり、インスタンスレベルのマルチモーダル情報を明示的に取り込むことで、3Dオブジェクト検出などのインスタンス中心のタスクを支援します。それは、階層的なScene Fusion（HSF）モジュールとInstance-Guided Fusion（IGF）モジュールから構成されます。HSFは、Point-to-GridおよびGrid-to-Regionトランスフォーマーを適用して、異なる粒度でのマルチモーダルなシーンコンテキストを捉えます。IGFはインスタンス候補を選定し、それらの関係を探索し、各インスタンスのローカルなマルチモーダルコンテキストを集約します。これらのインスタンスは、シーン特徴を強化し、インスタンスに対応するBEV表現を生み出すためのガイドとして機能します。難解なnuScenesベンチマークでは、IS-Fusionはこれまでに公開されているすべてのマルチモーダル作品を上回っています。コードはこちらで入手可能です：https://github.com/yinjunbo/IS-Fusion。
CaKDP: Category-aware Knowledge Distillation and Pruning Framework for Lightweight 3D Object Detection,Haonan Zhang · Longjun Liu · Yuqi Huang · YangZhao · Xinyu Lei · Bihan Wen,,,,,
SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection,Gang Zhang · Chen Junnan · Guohuan Gao · Jianmin Li · Si Liu · Xiaolin Hu,,,,,
Learning Occupancy for Monocular 3D Object Detection,Liang Peng · Junkai Xu · Haoran Cheng · Zheng Yang · Xiaopei Wu · Wei Qian · Wenxiao Wang · Boxi Wu · Deng Cai,,http://arxiv.org/abs/2305.15694v1,http://arxiv.org/pdf/2305.15694v1,"Monocular 3D detection is a challenging task due to the lack of accurate 3D information. Existing approaches typically rely on geometry constraints and dense depth estimates to facilitate the learning, but often fail to fully exploit the benefits of three-dimensional feature extraction in frustum and 3D space. In this paper, we propose \textbf{OccupancyM3D}, a method of learning occupancy for monocular 3D detection. It directly learns occupancy in frustum and 3D space, leading to more discriminative and informative 3D features and representations. Specifically, by using synchronized raw sparse LiDAR point clouds, we define the space status and generate voxel-based occupancy labels. We formulate occupancy prediction as a simple classification problem and design associated occupancy losses. Resulting occupancy estimates are employed to enhance original frustum/3D features. As a result, experiments on KITTI and Waymo open datasets demonstrate that the proposed method achieves a new state of the art and surpasses other methods by a significant margin. Codes and pre-trained models will be available at \url{https://github.com/SPengLiang/OccupancyM3D}.",モノキュラー3D検出は、正確な3D情報の欠如により難しい課題です。既存の手法は、一般的に幾何学的制約や密な深度推定に依存して学習を容易にしますが、しばしばフラスタムと3D空間での3次元特徴抽出の利点を十分に活用できません。本論文では、モノキュラー3D検出のための占有学習手法である\textbf{OccupancyM3D}を提案します。これは、フラスタムと3D空間での占有を直接学習し、より区別力があり情報量豊かな3D特徴と表現をもたらします。具体的には、同期した生の疎なLiDAR点群を使用して、空間の状態を定義し、ボクセルベースの占有ラベルを生成します。占有予測を簡単な分類問題として定式化し、関連する占有損失を設計します。得られた占有推定値は、元のフラスタム/3D特徴を強化するために使用されます。その結果、KITTIおよびWaymoオープンデータセットでの実験により、提案手法が最先端技術を実現し、他の手法を大幅に凌駕することが示されました。コードおよび事前学習済みモデルは以下のURLから入手できます：\url{https://github.com/SPengLiang/OccupancyM3D}。
Prompt3D: Random Prompt Assisted Weakly-Supervised 3D Object Detection,Xiaohong Zhang · Huisheng Ye · Jingwen Li · Qinyu Tang · Yuanqi Li · Yanwen Guo · Jie Guo,,,,,
MonoCD: Monocular 3D Object Detection with Complementary Depths,Longfei Yan · Pei Yan · Shengzhou Xiong · Xuanyu Xiang · Yihua Tan,,http://arxiv.org/abs/2404.03181v1,http://arxiv.org/pdf/2404.03181v1,"Monocular 3D object detection has attracted widespread attention due to its potential to accurately obtain object 3D localization from a single image at a low cost. Depth estimation is an essential but challenging subtask of monocular 3D object detection due to the ill-posedness of 2D to 3D mapping. Many methods explore multiple local depth clues such as object heights and keypoints and then formulate the object depth estimation as an ensemble of multiple depth predictions to mitigate the insufficiency of single-depth information. However, the errors of existing multiple depths tend to have the same sign, which hinders them from neutralizing each other and limits the overall accuracy of combined depth. To alleviate this problem, we propose to increase the complementarity of depths with two novel designs. First, we add a new depth prediction branch named complementary depth that utilizes global and efficient depth clues from the entire image rather than the local clues to reduce the correlation of depth predictions. Second, we propose to fully exploit the geometric relations between multiple depth clues to achieve complementarity in form. Benefiting from these designs, our method achieves higher complementarity. Experiments on the KITTI benchmark demonstrate that our method achieves state-of-the-art performance without introducing extra data. In addition, complementary depth can also be a lightweight and plug-and-play module to boost multiple existing monocular 3d object detectors. Code is available at https://github.com/elvintanhust/MonoCD.",モノキュラー3D物体検出は、一枚の画像からオブジェクトの3D位置を正確に取得する潜在能力を持ち、低コストで実現可能であるため、広く注目されています。奥行き推定は、2Dから3Dへのマッピングの不適切さから、モノキュラー3D物体検出の重要かつ難しいサブタスクです。さまざまな手法では、オブジェクトの高さやキーポイントなどの複数の局所的な奥行き手がを探索し、オブジェクトの奥行き推定を、単一の奥行き情報の不足を緩和するために複数の奥行き予測のアンサンブルとして定式化しています。しかし、既存の複数の奥行きの誤差は、同じ符号を持ちがちであり、それらが互いに中和するのを阻害し、組み合わせた奥行きの全体的な精度を制限しています。この問題を緩和するために、我々は奥行きの相補性を高めるために、2つの新しい設計を提案します。まず、局所的な手掛かりではなく画像全体からのグローバルで効率的な奥行き手がを利用する、相互補完的な奥行きという新しい深度予測ブランチを追加します。次に、複数の奥行き手間の幾何学的関係を十分に利用することで、形式上の相補性を実現します。これらの設計の恩恵を受けて、私たちの手法はより高い相補性を実現しています。KITTIベンチマークでの実験では、当社の手法が追加データを導入せずに最先端の性能を達成していることが示されています。また、相互補完的な奥行きは、既存のモノキュラー3D物体検出器を向上させるための軽量かつプラグアンドプレイなモジュールにもなり得ます。コードは https://github.com/elvintanhust/MonoCD で入手できます。
BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection,Zhenxin Li · Shiyi Lan · Jose M. Alvarez · Zuxuan Wu,,http://arxiv.org/abs/2312.01696v2,http://arxiv.org/pdf/2312.01696v2,"Recently, the rise of query-based Transformer decoders is reshaping camera-based 3D object detection. These query-based decoders are surpassing the traditional dense BEV (Bird's Eye View)-based methods. However, we argue that dense BEV frameworks remain important due to their outstanding abilities in depth estimation and object localization, depicting 3D scenes accurately and comprehensively. This paper aims to address the drawbacks of the existing dense BEV-based 3D object detectors by introducing our proposed enhanced components, including a CRF-modulated depth estimation module enforcing object-level consistencies, a long-term temporal aggregation module with extended receptive fields, and a two-stage object decoder combining perspective techniques with CRF-modulated depth embedding. These enhancements lead to a ""modernized"" dense BEV framework dubbed BEVNeXt. On the nuScenes benchmark, BEVNeXt outperforms both BEV-based and query-based frameworks under various settings, achieving a state-of-the-art result of 64.2 NDS on the nuScenes test set. Code will be available at \url{https://github.com/woxihuanjiangguo/BEVNeXt}.",最近、クエリベースのトランスフォーマーデコーダーの台頭により、カメラベースの3Dオブジェクト検出が変革されつつあります。これらのクエリベースのデコーダーは、従来の密なBEV（Bird's Eye View）ベースの手法を凌駕しています。しかし、私たちは、密なBEVフレームワークが、優れた深さ推定およびオブジェクトの位置特定能力を持つため、3Dシーンを正確かつ包括的に描写する点で重要であると主張します。本論文は、提案された改良されたコンポーネントを導入することで既存の密なBEVベースの3Dオブジェクト検出器の欠点に対処することを目的としています。これらの改良点には、オブジェクトレベルの整合性を強制するCRFモジュレーション深さ推定モジュール、拡張された受容野を持つ長期時間集積モジュール、およびCRFモジュレーション深度埋め込みを採用した、透視テクニックと組み合わせた2段階オブジェクトデコーダーが含まれています。これらの改良により、「モダナイズされた」密なBEVフレームワークであるBEVNeXtが生み出されます。nuScenesベンチマークにおいて、BEVNeXtはさまざまな設定で、BEVベースおよびクエリベースのフレームワークを凌駕し、nuScenesテストセットにおける64.2 NDSという最先端の結果を達成します。\url{https://github.com/woxihuanjiangguo/BEVNeXt} でコードが利用可能となります。
3DiffTection: 3D Object Detection with Geometry-aware Diffusion Features,Chenfeng Xu · Huan Ling · Sanja Fidler · Or Litany,,,,,
Commonsense Prototype for Outdoor Unsupervised 3D Object Detection,Hai Wu · Shijia Zhao · Xun Huang · Chenglu Wen · Xin Li · Cheng Wang,,http://arxiv.org/abs/2404.16493v1,http://arxiv.org/pdf/2404.16493v1,"The prevalent approaches of unsupervised 3D object detection follow cluster-based pseudo-label generation and iterative self-training processes. However, the challenge arises due to the sparsity of LiDAR scans, which leads to pseudo-labels with erroneous size and position, resulting in subpar detection performance. To tackle this problem, this paper introduces a Commonsense Prototype-based Detector, termed CPD, for unsupervised 3D object detection. CPD first constructs Commonsense Prototype (CProto) characterized by high-quality bounding box and dense points, based on commonsense intuition. Subsequently, CPD refines the low-quality pseudo-labels by leveraging the size prior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely scanned objects by the geometric knowledge from CProto. CPD outperforms state-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD), PandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD and testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on easy and moderate car classes, respectively. These achievements position CPD in close proximity to fully supervised detectors, highlighting the significance of our method. The code will be available at https://github.com/hailanyi/CPD.",非教師あり3D物体検出の主要なアプローチは、クラスターベースの擬似ラベル生成と反復的な自己学習プロセスに従います。しかし、LiDARスキャンのまばらさによる課題が生じ、誤ったサイズと位置を持つ疑似ラベルが生成され、検出性能が低下します。この問題に対処するため、本論文では非教師あり3D物体検出用のCommonsense Prototypeベースの検出器、CPDと呼ばれるものを紹介します。CPDは最初に、Commonsense Prototype（CProto）を構築し、高品質な境界ボックスと密な点で特徴付けます。その後、CPDはCProtoからのサイズ事前情報を活用して、低品質の擬似ラベルを改良します。さらに、CPDはCProtoからの幾何学的知識により、まばらにスキャンされた物体の検出精度を向上させます。CPDは、Waymo Open Dataset（WOD）、PandaSet、およびKITTIデータセットで、最先端の非教師あり3D検出器を大きく上回る性能を発揮します。また、WODでの訓練およびKITTIでのテストにより、CPDはそれぞれ容易な車両クラスと適度な車両クラスに対して、3D平均精度がそれぞれ90.85％、81.01％を達成します。これらの成果により、CPDは完全な教師あり検出器に近い位置に配置され、当社の手法の重要性が強調されます。コードはhttps://github.com/hailanyi/CPD で入手可能です。
HINTED: Hard Instance Enhanced Detector with Mixed-Density Feature Fusion for Sparsely-Supervised 3D Object Detection,Qiming Xia · Wei Ye · Hai Wu · Shijia Zhao · Leyuan Xing · Xun Huang · Jinhao Deng · Xin Li · Chenglu Wen · Cheng Wang,,,,,
Improving Distant 3D Object Detection Using 2D Box Supervision,Zetong Yang · Zhiding Yu · Christopher Choy · Renhao Wang · Anima Anandkumar · Jose M. Alvarez,,http://arxiv.org/abs/2403.09230v1,http://arxiv.org/pdf/2403.09230v1,"Improving the detection of distant 3d objects is an important yet challenging task. For camera-based 3D perception, the annotation of 3d bounding relies heavily on LiDAR for accurate depth information. As such, the distance of annotation is often limited due to the sparsity of LiDAR points on distant objects, which hampers the capability of existing detectors for long-range scenarios. We address this challenge by considering only 2D box supervision for distant objects since they are easy to annotate. We propose LR3D, a framework that learns to recover the missing depth of distant objects. LR3D adopts an implicit projection head to learn the generation of mapping between 2D boxes and depth using the 3D supervision on close objects. This mapping allows the depth estimation of distant objects conditioned on their 2D boxes, making long-range 3D detection with 2D supervision feasible. Experiments show that without distant 3D annotations, LR3D allows camera-based methods to detect distant objects (over 200m) with comparable accuracy to full 3D supervision. Our framework is general, and could widely benefit 3D detection methods to a large extent.","遠隔の3Dオブジェクトの検出の向上は重要かつ困難なタスクです。カメラベースの3D認識において、3D境界ボックスのアノテーションは正確な深度情報のためにLiDARに大きく依存しています。そのため、遠距離のオブジェクトのLiDARポイントのまばらさにより、アノテーションの距離が制限されることが多く、既存の検出器が長距離シナリオのための機能を阻害しています。私たちは、遠隔のオブジェクトについては簡単にアノテーションできるため、遠隔オブジェクトに対して2Dボックスの監督のみを考慮することでこの課題に取り組んでいます。私たちはLR3Dという、遠隔のオブジェクトの欠損深度を回復することを学習するフレームワークを提案しています。LR3Dは、2Dボックスと深度の間のマッピングの生成を学習するために暗黙の射影ヘッドを採用し、近距離のオブジェクトに対する3D監督を使用します。このマッピングにより、2Dボックスに依存した遠隔オブジェクトの深度推定が可能となり、2D監督に基づく長距離3D検出が可能となります。実験結果では、遠隔の3DアノテーションなしでLR3Dを使用すると、カメラベースの方法が遠隔のオブジェクト（200m以上）を完全な3D監督に匹敵する精度で検出できることが示されます。私たちのフレームワークは一般的であり、広範囲にわたる3D検出方法に多大な利益をもたらす可能性があります。2022年10月4日
